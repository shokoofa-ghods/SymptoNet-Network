{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import numpy as np\n",
    "import csv \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_crawler(url):\n",
    "    # Send a GET request to the URL\n",
    "    session = requests.Session()\n",
    "    response = session.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    # Parse the HTML content of the response using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main URL\n",
    "url = 'https://en.wikipedia.org/wiki/Lists_of_diseases'\n",
    "\n",
    "soup = web_crawler(url)\n",
    "section1_links = soup.find('div', {'class':'mw-content-ltr mw-parser-output'}).find_all('a', href=True)[32:49]\n",
    "origin = 'https://en.wikipedia.org/'\n",
    "\n",
    "urls = [origin + section1_links[i]['href'] for i in range(len(section1_links))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_symptom(url_disease_links):\n",
    "    dict = {}\n",
    "    has_symptom = None\n",
    "\n",
    "    for url_disease_link in url_disease_links:\n",
    "        symptoms = list()\n",
    "        soup = web_crawler(url_disease_link)\n",
    "        has_symptom = soup.find('span', { 'class':'mw-headline', 'id': 'Signs_and_symptoms'})\n",
    "        if has_symptom:\n",
    "            for element in has_symptom.parent.next_siblings:\n",
    "                if element.name == 'p':\n",
    "                    symptoms.append(element.get_text())\n",
    "                if element.name == 'h2':\n",
    "                    break\n",
    "            disease = url_disease_link.split('/')[-1]\n",
    "            dict[f'{disease}'] = symptoms\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(url_i_disease_links, i):\n",
    "    fname = 'url_i_disease_links'.replace('i', f'{i}', 1)\n",
    "    with open(f'{fname}.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "        wrtr = csv.writer(f)\n",
    "        wrtr.writerow(['disease', 'symptoms'])\n",
    "        dict = extract_symptom(url_i_disease_links)\n",
    "        for k,v in dict.items():\n",
    "            wrtr.writerow([k,v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_0_disease_links = list()\n",
    "soup = web_crawler(urls[0])\n",
    "for i in range (1,132):\n",
    "    t = soup.find('div', {'class': \"mw-content-container\"}).find_all('tr')[i].find('a', href=True)\n",
    "    if t != None:\n",
    "        url_0_disease_links.append(origin + t['href'])\n",
    "\n",
    "write_file(url_0_disease_links, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1_disease_links = list()\n",
    "soup = web_crawler(urls[1])\n",
    "\n",
    "for i in soup.find_all('h2'):\n",
    "    next_ul = i.find_next_sibling('ul')\n",
    "    if next_ul:\n",
    "        links = next_ul.find_all('a', href=True)\n",
    "        for a in links:\n",
    "            if a and a['href']:\n",
    "                url_1_disease_links.append(origin + a['href'])\n",
    "   \n",
    "write_file(url_1_disease_links, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_2_disease_links = list()\n",
    "soup = web_crawler(urls[2])\n",
    "\n",
    "list_siblings = soup.find_all('h2')[1].find_next_siblings()\n",
    "for link in list_siblings[4].find_all('a') + list_siblings[7].find_all('a'):\n",
    "    url_2_disease_links.append(origin + link['href'])\n",
    "\n",
    "write_file(url_2_disease_links, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_3_disease_links = list()\n",
    "soup = web_crawler(urls[3])\n",
    "\n",
    "diseases_section = soup.find(id='List_of_diseases')\n",
    "if diseases_section:\n",
    "        list_items = diseases_section.find_all_next('li')\n",
    "        for li in list_items:\n",
    "            a_tags = li.find_all('a')\n",
    "            for a in a_tags:\n",
    "                if 'href' in a.attrs:\n",
    "                    url_3_disease_links.append(origin + a['href']) \n",
    "\n",
    "for link in url_3_disease_links[:150]:\n",
    "    if len(link.split('#')) >1 :\n",
    "        url_3_disease_links.remove(link)\n",
    "\n",
    "write_file(url_3_disease_links, 3)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_5_disease_links = list()\n",
    "soup = web_crawler(urls[5])\n",
    "\n",
    "starting_point = soup.find('span', id='H00-H06_Disorders_of_eyelid,_lacrimal_system_and_orbit')\n",
    "if starting_point:\n",
    "    # Find the next unordered list (ul) that contains the list items (li)\n",
    "    current_point = starting_point.find_next('ul')\n",
    "    while current_point and current_point.name == 'ul':\n",
    "        for li in current_point.find_all('li', recursive=True):  # Change to True to get nested lists\n",
    "            a_tag = li.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                url_5_disease_links.append(origin + a_tag['href'])\n",
    "        \n",
    "        current_point = current_point.find_next_sibling()\n",
    "\n",
    "        # Check if we have reached the section containing \"Argyll Robertson pupil\"\n",
    "        if any(\"Argyll Robertson pupil\" in s for s in current_point.stripped_strings):\n",
    "            # Capture the link for \"Argyll Robertson pupil\"\n",
    "            a_tag = current_point.find('a', string=\"Argyll Robertson pupil\")\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                full_url = f\"https://en.wikipedia.org{a_tag['href']}\"\n",
    "                url_5_disease_links.append(full_url)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_section_id = 'H00-H06_Disorders_of_eyelid,_lacrimal_system_and_orbit'\n",
    "end_section_id = 'H55-H59_Other_disorders_of_eye_and_adnexa'\n",
    "\n",
    "starting_point = soup.find(id=start_section_id)\n",
    "\n",
    "if starting_point:\n",
    "    # Find the next unordered list (ul) that contains the list items (li)\n",
    "    current_point = starting_point.find_next('ul')\n",
    "    while current_point and current_point.name == 'ul':\n",
    "        for li in current_point.find_all('li', recursive=True):\n",
    "            a_tag = li.find('a')\n",
    "            url_5_disease_links.append(origin + a_tag['href'])\n",
    "\n",
    "        current_point = current_point.find_next_sibling()\n",
    "\n",
    "        # Check if we have reached the end section id\n",
    "        if current_point and current_point.find(id=end_section_id):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "start_section_id = 'H00-H06_Disorders_of_eyelid,_lacrimal_system_and_orbit'\n",
    "end_section_id = 'H30-H36_Disorders_of_choroid_and_retina'\n",
    "\n",
    "start_section = soup.find(id=start_section_id)\n",
    "end_section = soup.find(id=end_section_id)\n",
    "\n",
    "if start_section and end_section:\n",
    "    content = start_section.find_next('ul').find_all('li', recursive=True)  # Changed to include sub-items\n",
    "\n",
    "    for li in content:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            full_url = f\"https://en.wikipedia.org{a_tag['href']}\"\n",
    "            url_5_disease_links.append(full_url)\n",
    "\n",
    "        if li.find_next_sibling(['h2', 'h3']) is not None:\n",
    "            next_header = li.find_next_sibling(['h2', 'h3'])\n",
    "            if next_header and end_section_id in next_header.get('id', ''):\n",
    "                break\n",
    "\n",
    "print(len(url_5_disease_links))\n",
    "\n",
    "write_file(url_5_disease_links, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_8_disease_links = list()\n",
    "soup = web_crawler(urls[8])\n",
    "for i in range(71):\n",
    "    t = soup.find('div', {'class': \"mw-content-ltr mw-parser-output\"}).find_all('tr')[i].find('a', href=True)\n",
    "    if t != None:\n",
    "        url_8_disease_links.append(origin + t['href'])\n",
    "\n",
    "write_file(url_8_disease_links, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_16_disease_links = list()\n",
    "soup = web_crawler(urls[16])\n",
    "\n",
    "for i in range(243):\n",
    "    t = soup.find('div', {'class': \"mw-content-ltr mw-parser-output\"}).find_all('tr')[i].find('a', href=True)\n",
    "    if t != None:\n",
    "        url_16_disease_links.append(origin + t['href'])\n",
    "\n",
    "write_file(url_16_disease_links, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(url_16_disease_links, 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
